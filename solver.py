import os,sys,time
from datetime import datetime
from os.path import join
import torch
from torch import optim
from torch.optim.lr_scheduler import StepLR
from torch.cuda.amp import GradScaler, autocast
from torchmetrics import Accuracy, JaccardIndex
from torchmetrics.classification import Specificity, Precision, Recall,AveragePrecision, AUROC  # recall==sensitivity
from logs.logger import Print_Logger
from models.UNetFamily import UNet,ResUNet
from PIL import Image
from tools import concat_result

class Solver(object):
    def __init__(self, config, train_loader, valid_loader, test_loader):
        self.args=config
        self.early_stop = config.early_stop
        self.unet = None
        self.optimizer = None
        self.device = config.device
        # AMP
        self.scaler = GradScaler()
        self.autocast = autocast
        # loaders
        self.train_loader = train_loader
        self.valid_loader = valid_loader
        self.test_loader = test_loader

        # params
        self.model_path = config.model_path
        self.result_path = config.result_path
        self.lr = config.lr
        self.beta1 = config.beta1
        self.beta2 = config.beta2
        self.num_epochs = config.num_epochs
        self.num_epochs_decay = config.num_epochs_decay
        self.model_type = config.model_type
        self.t = getattr(config, 't', 2)

        # loss for 2-class segmentation
        self.criterion = torch.nn.CrossEntropyLoss()

        # Metrics for multiclass (2 classes: background vs vessel)
        self.metric_acc = Accuracy(task="binary", threshold=0.5).to(self.device)
        self.metric_se  = Recall(task="binary", threshold=0.5).to(self.device)
        self.metric_sp  = Specificity(task="binary", threshold=0.5).to(self.device)
        self.metric_pc  = Precision(task="binary", threshold=0.5).to(self.device)
        self.metric_js  = JaccardIndex(task="binary", threshold=0.5).to(self.device)
        self.metric_auprc = AveragePrecision(task="binary").to(self.device)
        self.metric_auroc = AUROC(task="binary").to(self.device)
        # build
        self.build_model()
        self.optimizer = optim.Adam(self.unet.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))
        self.scheduler = StepLR(self.optimizer, step_size=self.num_epochs_decay, gamma=0.1)

    def build_model(self):
        # output_ch=2 for background and vessel
        if self.model_type == 'UNet':
            model = UNet(1,2)
        else:
            model =ResUNet(1,2)

        model.to(self.device)
        self.unet = model
        print(f"train {self.model_type}")

    def reset_grad(self):
        self.optimizer.zero_grad()

    def _run_epoch(self, loader, is_train=True, epoch=None):
        # è®¾ç½®æ¨¡å‹æ¨¡å¼
        if is_train:
            self.unet.train()
        else:
            self.unet.eval()

        running_loss = 0.0
        # é‡ç½®æŒ‡æ ‡
        for m in (self.metric_acc, self.metric_se, self.metric_sp,
        self.metric_pc, self.metric_js,
        self.metric_auprc, self.metric_auroc):
            m.reset()

        # å¦‚æœæ˜¯éªŒè¯æ¨¡å¼ï¼Œåˆ›å»ºç»“æœä¿å­˜ç›®å½•
        if not is_train:
            result_dir = join(self.result_path, f'epoch_{epoch + 1}')
            os.makedirs(result_dir, exist_ok=True)

        # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆå¯æ ¹æ®æ˜¾å­˜å’Œéœ€æ±‚è°ƒæ•´ï¼‰
        accumulation_steps = 4  # ä¾‹å¦‚ç´¯ç§¯4ä¸ªæ‰¹æ¬¡

        for i, (images, GT) in enumerate(loader):
            images = images.to(self.device)
            gt_idx = GT.squeeze(1).long().to(self.device)

            if is_train:
                with self.autocast():
                    logits = self.unet(images)
                    loss = self.criterion(logits, gt_idx)
                running_loss += loss.item() * images.size(0)

                # æ›´æ–°æŒ‡æ ‡ç”¨ fp32 é¢„æµ‹
                probs = torch.softmax(logits, dim=1)
                preds = torch.argmax(logits, dim=1)
                self.metric_acc.update(preds, gt_idx)
                self.metric_se.update(preds, gt_idx)
                self.metric_sp.update(preds, gt_idx)
                self.metric_pc.update(preds, gt_idx)
                self.metric_js.update(preds, gt_idx)
                self.metric_auprc.update(probs[:, 1].flatten(), gt_idx.flatten())
                self.metric_auroc.update(probs[:, 1].flatten(), gt_idx.flatten())

                # ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­ï¼Œç´¯ç§¯æ¢¯åº¦
                self.scaler.scale(loss).backward()

                # åœ¨ç´¯ç§¯æ­¥æ•°è¾¾åˆ°æ—¶æ›´æ–°å‚æ•°å¹¶é‡ç½®æ¢¯åº¦
                if (i + 1) % accumulation_steps == 0 or (i + 1) == len(loader):
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.reset_grad()
            else:
                # éªŒè¯æ¨¡å¼ä¸‹åªéœ€å‰å‘ä¼ æ’­
                logits = self.unet(images)
                probs = torch.softmax(logits, dim=1)
                preds = torch.argmax(logits, dim=1)
                self.metric_acc.update(preds, gt_idx)
                self.metric_se.update(preds, gt_idx)
                self.metric_sp.update(preds, gt_idx)
                self.metric_pc.update(preds, gt_idx)
                self.metric_js.update(preds, gt_idx)
                self.metric_auprc.update(probs[:, 1].flatten(), gt_idx.flatten())
                self.metric_auroc.update(probs[:, 1].flatten(), gt_idx.flatten())

                loss = self.criterion(logits, gt_idx)
                running_loss += loss.item() * images.size(0)
                self._save_predictions(images, logits, GT, result_dir, i)

        # è®¡ç®—æŒ‡æ ‡
        n = len(loader.dataset)
        epoch_loss = running_loss / n
        se = self.metric_se.compute()
        pc = self.metric_pc.compute()
        dice_score = 2 * (se * pc) / (se + pc + 1e-6)

        results = {
            'loss': epoch_loss,
            'acc': self.metric_acc.compute().item(),
            'SP': self.metric_sp.compute().item(),
            'JS': self.metric_js.compute().item(),
            'AUPRC': self.metric_auprc.compute().item(),
            'AUROC': self.metric_auroc.compute().item(),
            'SE': se.item(),
            'PC': pc.item(),
            'DC': dice_score.item()
        }
        return results

    def train_epoch(self, loader):
        """è®­ç»ƒä¸€ä¸ª epoch"""
        return self._run_epoch(loader, is_train=True)

    def validate_epoch(self, loader, epoch):
        """éªŒè¯ä¸€ä¸ª epoch"""
        return self._run_epoch(loader, is_train=False, epoch=epoch)

    @staticmethod
    def _save_predictions(images, logits, GT, result_dir, batch_idx):
        # æ¢å¤ä¸ºå¤šç±»æ¦‚ç‡
        probs = torch.softmax(logits, dim=1)
        for idx in range(images.size(0)):
            # è·å–åŸå§‹å›¾åƒ
            ori_img = images[idx].cpu().detach().numpy().transpose(1, 2, 0)
            # å¦‚æœ ori_img æ˜¯ [0,1] èŒƒå›´ï¼Œè½¬æ¢ä¸º [0,255]
            if ori_img.max() <= 1:
                ori_img = ori_img * 255

            # è·å–é¢„æµ‹ç»“æœï¼ˆè¡€ç®¡æ¦‚ç‡ï¼‰
            pred_res = probs[idx, 1].cpu().detach().numpy()  # (H, W)

            # è·å–çœŸå®æ ‡ç­¾
            gt = GT[idx].squeeze(0).cpu().detach().numpy()  # (H, W)

            # æ‹¼æ¥å›¾åƒ
            total_img = concat_result(ori_img, pred_res, gt)
            if total_img.ndim == 3 and total_img.shape[2] == 1:
                # å•é€šé“å›¾åƒï¼Œç§»é™¤é€šé“ç»´åº¦å¹¶æŒ‡å®šæ¨¡å¼ä¸º 'L'
                total_img = total_img.squeeze(2)  # å½¢çŠ¶ä» (H, 4*W, 1) å˜ä¸º (H, 4*W)
                total_pil = Image.fromarray(total_img, mode='L')
            else:
                # å¤šé€šé“å›¾åƒï¼ˆä¾‹å¦‚ RGBï¼‰ï¼Œç›´æ¥è½¬æ¢
                total_pil = Image.fromarray(total_img)  # é»˜è®¤æ¨¡å¼ä¸º 'RGB'

            # ä¿å­˜æ‹¼æ¥å›¾åƒ
            total_pil.save(join(result_dir, f'batch_{batch_idx}_img_{idx}_concat.png'))

    def train(self,args):
        # è·å–å½“å‰æ—¥æœŸå¹¶ç”Ÿæˆæ–‡ä»¶å
        today = datetime.now().strftime("%m%d")
        log_path = './logs'  #æ—¥å¿—è·¯å¾„
        log_filename = f"{self.model_type}_{today}r.log"
        sys.stdout = Print_Logger(os.path.join(log_path, log_filename))
        print("The computing device used is:" + self.device)

        #å¦‚æœä¿å­˜äº†æ¨¡å‹æƒé‡ï¼Œåˆ™è¿›è¡Œé¢„è®­ç»ƒ
        if args.pre_trained:
            # Load checkpoint.   checkpointç›¸å½“äºä¿å­˜æ¨¡å‹çš„å‚æ•°ï¼Œä¼˜åŒ–å™¨å‚æ•°ï¼Œlossï¼Œepochçš„æ–‡ä»¶å¤¹
            print('==> Resuming from checkpoint..')
            checkpoint = torch.load(join(self.model_path,f"{self.model_type}_latest.pth"),
                map_location=args.device)
            self.unet.load_state_dict(checkpoint['net'])
            self.optimizer.load_state_dict(checkpoint['optimizer'])
            args.start_epoch = checkpoint['epoch'] + 1

        best = {'epoch': 0, 'score': 0.5}
        trigger = 0                                             #åˆå§‹åŒ–æ—©åœç´¯åŠ å™¨
        for epoch in range(args.start_epoch, self.num_epochs+1):
            train_res = self.train_epoch(self.train_loader)
            val_res   = self.validate_epoch(self.valid_loader, epoch)

            self.scheduler.step()

            #æ‰“å°å½“å‰epochã€å­¦ä¹ ç‡å’Œæ—¶é—´
            print('\nEPOCH: %d/%d --(learn_rate:%.6f) | Time: %s' % \
                  (epoch, self.num_epochs, self.optimizer.state_dict()['param_groups'][0]['lr'],time.asctime()))

            print(f"[Train] Loss: {train_res['loss']:.4f}  SE: {train_res['SE']:.4f}  SP: {train_res['SP']:.4f}  ACC: {train_res['acc']:.4f}  "
                  f"F1: {train_res['DC']:.4f}  PC: {train_res['PC']:.4f}  IOU: {train_res['JS']:.4f}  PR: {train_res['AUPRC']:.4f}  AUROC: {train_res['AUROC']:.4f}")
            print(f"[Valid] Loss: {val_res['loss']:.4f}  SE: {val_res['SE']:.4f}  SP: {val_res['SP']:.4f}  ACC: {val_res['acc']:.4f}  "
                  f"F1: {val_res['DC']:.4f}  PC: {val_res['PC']:.4f}  IOU: {val_res['JS']:.4f}  PR: {val_res['AUPRC']:.4f}  AUROC: {val_res['AUROC']:.4f}")

            #ä¿å­˜å½“å‰epochçš„æ¨¡å‹ï¼ˆæƒé‡ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œepochï¼Œå¯ä»¥åœ¨ä¸‹ä¸€æ¬¡æ¥ç€è®­ç»ƒï¼‰
            state = {'net': self.unet.state_dict(), 'optimizer': self.optimizer.state_dict(), 'epoch': epoch}
            torch.save(state, join(self.model_path,f"{self.model_type}_latest.pth"))
            trigger += 1
            score =  0.5*val_res['AUPRC']+ 0.5*val_res['DC']        #IOUã€SEå’ŒF1åˆ†åˆ«å æ¯”ä¸º0.2ã€0.3å’Œ0.5

            if score > best['score']:
                best['score'] = score
                best['epoch'] = epoch
                trigger = 0
                torch.save(state, join(self.model_path, f"{self.model_type}_best.pth"))
                print(f"ğŸ“Œ Best model saved at epoch {epoch}, score={best['score']:.4f}")

            if trigger >= self.early_stop:
                print("=> early stopping")
                break

        torch.save(state, join(self.model_path, f"{self.model_type}_best.pth"))

    def test(self):
        """
        æµ‹è¯•æ¥å£ï¼šåœ¨ test_loader ä¸Šè¯„ä¼°å·²ä¿å­˜çš„æœ€ä½³æ¨¡å‹ï¼Œå¹¶ä¿å­˜é¢„æµ‹å›¾ä¸æŒ‡æ ‡ã€‚
        args.device åº”ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼Œä¾‹å¦‚ 'cuda:0' æˆ– 'cpu'ã€‚
        """
        # 1. åŠ è½½æœ€ä¼˜æ¨¡å‹å‚æ•°
        best_path = join(self.model_path, f"{self.model_type}_latest.pth")
        assert os.path.exists(best_path), f"æ‰¾ä¸åˆ°æœ€ä½³æ¨¡å‹æ–‡ä»¶: {best_path}"
        print(f"==> Loading best model from {best_path}")
        checkpoint = torch.load(best_path, map_location=self.device)
        # å¦‚æœä¿å­˜æ—¶åªä¿å­˜äº† state_dictï¼Œç›´æ¥ load_state_dict(...)
        # å¦‚æœåƒè®­ç»ƒæ—¶ä¿å­˜äº† {'net':state_dict,...}
        if isinstance(checkpoint, dict) and 'net' in checkpoint:
            self.unet.load_state_dict(checkpoint['net'])
        else:
            self.unet.load_state_dict(checkpoint)
        self.unet.to(self.device)
        self.unet.eval()

        # 2. é‡ç½®æ‰€æœ‰æŒ‡æ ‡
        for m in (self.metric_acc, self.metric_se, self.metric_sp,
                  self.metric_pc, self.metric_js,
                  self.metric_auprc, self.metric_auroc):
            m.reset()

        # 3. åˆ›å»ºæµ‹è¯•ç»“æœä¿å­˜ç›®å½•
        test_dir = join(self.result_path, "test")
        os.makedirs(test_dir, exist_ok=True)

        # 4. éå† test_loaderï¼Œä¸è®¡ç®—æ¢¯åº¦ï¼Œä¿å­˜æ¯ä¸ª batch çš„é¢„æµ‹å›¾
        total_loss = 0.0
        with torch.no_grad():
            for i, (images, GT) in enumerate(self.test_loader):
                images = images.to(self.device)
                gt_idx = GT.squeeze(1).long().to(self.device)

                logits = self.unet(images)               # [B,2,H,W]
                probs  = torch.softmax(logits, dim=1)    # [B,2,H,W]
                preds  = torch.argmax(logits, dim=1)     # [B,H,W]

                # æ›´æ–°æŒ‡æ ‡
                self.metric_acc.update(preds, gt_idx)
                self.metric_se .update(preds, gt_idx)
                self.metric_sp .update(preds, gt_idx)
                self.metric_pc .update(preds, gt_idx)
                self.metric_js .update(preds, gt_idx)
                self.metric_auprc.update(probs[:,1].flatten(), gt_idx.flatten())
                self.metric_auroc.update(probs[:,1].flatten(), gt_idx.flatten())

                # ç´¯è®¡æŸå¤±
                loss = self.criterion(logits, gt_idx)
                total_loss += loss.item() * images.size(0)

                # ä¿å­˜è¯¥ batch çš„é¢„æµ‹å¯è§†åŒ–
                self._save_predictions(images, logits, GT, test_dir, i)

        # 5. è®¡ç®—å¹¶æ‰“å°æµ‹è¯•é›†æ•´ä½“æŒ‡æ ‡
        n = len(self.test_loader.dataset)
        avg_loss = total_loss / n
        se  = self.metric_se.compute()
        pc  = self.metric_pc.compute()
        dice= 2 * (se * pc) / (se + pc + 1e-6)

        results = {
            'loss':  avg_loss,
            'acc':   self.metric_acc.compute().item(),
            'SE':    se.item(),
            'SP':    self.metric_sp.compute().item(),
            'PC':    pc.item(),
            'JS':    self.metric_js.compute().item(),
            'DC':    dice.item(),
            'AUPRC': self.metric_auprc.compute().item(),
            'AUROC': self.metric_auroc.compute().item(),
        }

        print("===== Test Results =====")
        print(f"Loss:  {results['loss']:.4f}")
        print(f"ACC:   {results['acc']:.4f}")
        print(f"SE:    {results['SE']:.4f}")
        print(f"SP:    {results['SP']:.4f}")
        print(f"PC:    {results['PC']:.4f}")
        print(f"JS:    {results['JS']:.4f}")
        print(f"DC(F1):{results['DC']:.4f}")
        print(f"AUPRC: {results['AUPRC']:.4f}")
        print(f"AUROC: {results['AUROC']:.4f}")

        # è¿”å›ç»“æœå­—å…¸ï¼Œä¾›å¤–éƒ¨è„šæœ¬è¿›ä¸€æ­¥ä½¿ç”¨
        return results